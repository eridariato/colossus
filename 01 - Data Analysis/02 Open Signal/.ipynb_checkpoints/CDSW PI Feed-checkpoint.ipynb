{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import HiveContext, SparkContext, SparkConf, SQLContext, Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, TimestampType, StructType, StructField, StringType, MapType, FloatType\n",
    "import pyspark.sql.types as t\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .setMaster(\"yarn-client\")\n",
    "        .setAppName(\"athena - open signal\")\n",
    "        .set(\"spark.executor.memory\", \"16g\")\n",
    "        .set(\"spark.executor.cores\", \"1\")\n",
    "        .set(\"spark.driver.memory\", \"16g\")\n",
    "        .set(\"spark.yarn.queue\", \"root.hue_dmp\")\n",
    "        .set(\"spark.default.parallelism\", \"8\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "        .set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .set(\"spark.yarn.driver.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.yarn.executor.memoryOverhead\", \"4096\")\n",
    "        .set(\"spark.kryoserializer.buffer.max\", \"1g\")\n",
    "        .set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
    "        .set(\"spark.dynamicAllocation.maxExecutors\", \"5\")\n",
    "        .set(\"spark.driver.maxResultSize\", \"8g\")\n",
    "        .set(\"spark.dynamicAllocation.initialExecutors\", \"1\")\n",
    "        .set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\")\n",
    "        .set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "        .set(\"spark.sql.crossJoin.enabled\", \"true\"))\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "hiveContext = HiveContext(sc)\n",
    "spark_session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing To HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_download = [\n",
    "    'categorydownload_4g_mean',\n",
    "    'categorydownload_4g_lci',\n",
    "    'categorydownload_4g_uci',\n",
    "    'categorypeakdownload_4g_estimate',\n",
    "    'categorypeakdownload_4g_lci',\n",
    "    'categorypeakdownload_4g_uci',\n",
    "    'androidmodeldownload_4g_mean',\n",
    "    'androidmodeldownload_4g_lci',\n",
    "    'androidmodeldownload_4g_uci',\n",
    "    'androidmodelpeakdownload_4g_estimate',\n",
    "    'androidmodelpeakdownload_4g_lci',\n",
    "    'androidmodelpeakdownload_4g_uci',\n",
    "    'iosmodeldownload_4g_mean',\n",
    "    'iosmodeldownload_4g_lci',\n",
    "    'iosmodeldownload_4g_uci',\n",
    "    'iosmodelpeakdownload_4g_estimate',\n",
    "    'iosmodelpeakdownload_4g_lci',\n",
    "    'iosmodelpeakdownload_4g_uci',\n",
    "]\n",
    "\n",
    "items_upload = [\n",
    "    'categoryupload_4g_mean',\n",
    "    'categoryupload_4g_lci',\n",
    "    'categoryupload_4g_uci',\n",
    "    'categorypeakupload_4g_estimate',\n",
    "    'categorypeakupload_4g_lci',\n",
    "    'categorypeakupload_4g_uci',\n",
    "    'androidmodelupload_4g_mean',\n",
    "    'androidmodelupload_4g_lci',\n",
    "    'androidmodelupload_4g_uci',\n",
    "    'androidmodelpeakupload_4g_estimate',\n",
    "    'androidmodelpeakupload_4g_lci',\n",
    "    'androidmodelpeakupload_4g_uci',\n",
    "    'iosmodelupload_4g_mean',\n",
    "    'iosmodelupload_4g_lci',\n",
    "    'iosmodelupload_4g_uci',\n",
    "    'iosmodelpeakupload_4g_estimate',\n",
    "    'iosmodelpeakupload_4g_lci',\n",
    "    'iosmodelpeakupload_4g_uci',\n",
    "]\n",
    "\n",
    "items_latency = [\n",
    "    'categorylatency_4g_mean',\n",
    "    'categorylatency_4g_lci',\n",
    "    'categorylatency_4g_uci',\n",
    "    'categorypeaklatency_4g_estimate',\n",
    "    'categorypeaklatency_4g_lci',\n",
    "    'categorypeaklatency_4g_uci',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_device(\n",
    "    df: pyspark.sql.DataFrame,\n",
    "    items: list,\n",
    "    subject: str,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "    \n",
    "    for item in items:\n",
    "        df = df.withColumn(f'{item}_value', F.array(F.expr(f\"speed.{subject}.{item}.*\")))\n",
    "        df = df.withColumn(\n",
    "            f'{item}_key',\n",
    "            F.array([F.lit(field.name) for field in next(field for field in df.select(F.col(f'speed.{subject}.{item}')).schema.fields).dataType.fields])\n",
    "        ).withColumn(\n",
    "            f'{item}', \n",
    "            F.map_from_arrays(f'{item}_key', f'{item}_value')\n",
    "        ).drop(f'{item}_key').drop(f'{item}_value')\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_device(\n",
    "    filename:str,\n",
    "    filedir: str,\n",
    "    weekstart: str,\n",
    ") -> pyspark.sql.DataFrame:\n",
    "\n",
    "    filename = \"pi_feed_devices_all_countries_telkomselidn_20200420_20200520.json.gz\"\n",
    "    filedate = \"2020/04/20/\"\n",
    "    filedir = \"hdfs:///data/landing/gx_pnt/eri/08_data/ci_pi_feeds/pi_feed_devices/v1.0/\"\n",
    "    file = filedir + filedate + filename\n",
    "    \n",
    "    for item in items:\n",
    "        df = df.withColumn(f'{item}_value', F.array(F.expr(f\"speed.{subject}.{item}.*\")))\n",
    "        df = df.withColumn(\n",
    "            f'{item}_key',\n",
    "            F.array([F.lit(field.name) for field in next(field for field in df.select(F.col(f'speed.{subject}.{item}')).schema.fields).dataType.fields])\n",
    "        ).withColumn(\n",
    "            f'{item}', \n",
    "            F.map_from_arrays(f'{item}_key', f'{item}_value')\n",
    "        ).drop(f'{item}_key').drop(f'{item}_value')\n",
    "    return df\n",
    "\n",
    "    df_a = transform_device(df, items_download, subject='download')\n",
    "    df_b = transform_device(df_a, items_upload, subject='upload')\n",
    "    df_c = transform_device(df_b, items_latency, subject='latency')\n",
    "\n",
    "    df_final = (df_c\n",
    "                .withColumn('level', F.lit('countries'))\n",
    "                .withColumn('weekstart', F.lit('2020-05-20'))\n",
    "                .drop('speed')\n",
    "               )\n",
    "\n",
    "device_path = \"hdfs:///data/landing/gx_pnt/eri/08_data/os_data/pi_device_countries\"\n",
    "df_final.coalesce(1).write.mode(\"overwrite\").format(\"parquet\").partitionBy('weekstart').parquet(device_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_path = \"hdfs:///data/landing/gx_pnt/eri/08_data/os_data/pi_device_countries\"\n",
    "df = spark_session.read.parquet(device_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"dbi_so.os_pi_device\"\n",
    "df_final.coalesce(1).write.mode(\"overwrite\").format(\"parquet\").partitionBy('weekstart').saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest in peace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
